{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def my_logistic_regression(df_competition):\n",
    "    \"\"\"\n",
    "    Computes\n",
    "    \"\"\"\n",
    "\n",
    "    # format data\n",
    "    data = df_competition.values  # convert data frame to numpy array\n",
    "    m, n = data[:, :-1].shape\n",
    "    X = np.concatenate((np.ones((m, 1)), data[:, :-1]), axis=1)\n",
    "    y = data[:, -1]\n",
    "\n",
    "    # optimize theta\n",
    "    initial_theta = np.zeros(X.shape[1])\n",
    "    res = minimize(costFunction, initial_theta, args=(X, y),\n",
    "               method=None, jac=gradient, options={'maxiter': 1000})\n",
    "    return res.x\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Computes the sigmoid function for all values of z.\n",
    "\n",
    "    :param z: scalar, vector or array of values for which the sigmoid function should be computed\n",
    "    :returns: scalar, vector or array of values of size z containing computed sigmoid values\n",
    "    \"\"\"\n",
    "    return (1 / (1 + np.power(np.e, -z)))\n",
    "\n",
    "\n",
    "def costFunction(theta, X, y):\n",
    "    \"\"\"\n",
    "    Computes the cost of using theta as the parameter for logistic regression and the\n",
    "    gradient of the cost w.r.t. to the parameters\n",
    "\n",
    "    :param theta: parameter vector containing corresponding values of theta\n",
    "    :param X: MxN array that contains feature set (x-values)\n",
    "    :param y: Mx1 array that contains resulting outcomes (y-values)\n",
    "    :returns: a scalar of the cost \"J\" and gradient \"grad\" of the cost with the same size as theta\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    h = sigmoid(np.dot(X, theta))  # get predictions\n",
    "    J = -(1 / m) * (np.log(h).T.dot(y) +\n",
    "                    np.log(1 - h).T.dot(1 - y))  # calculate cost\n",
    "\n",
    "    return J\n",
    "\n",
    "\n",
    "def gradient(theta, X, y):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the cost with respect to cost J and theta.\n",
    "\n",
    "    :param theta: parameter vector containing corresponding values of theta\n",
    "    :param X: MxN array that contains feature set (x-values)\n",
    "    :param y: Mx1 array that contains resulting outcomes (y-values)\n",
    "    :returns: gradient of the cost with the same size as theta\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    h = sigmoid(np.dot(X, theta))\n",
    "    return (1 / m) * np.dot(X.transpose(), h - y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_competition_evaluation = pd.read_csv('C:/Users/802161/Source/github/glm-competition/data/df_competition.csv')\n",
    "\n",
    "# listed of average AUCs\n",
    "AUC_avgs = []\n",
    "\n",
    "for avg in range(0, 10):\n",
    "    # list of calculated AUCs\n",
    "    AUC_list = []\n",
    "    \n",
    "    for iteration in range(0, 100):\n",
    "        n = len(df_competition_evaluation)\n",
    "\n",
    "        # divide into training and test\n",
    "        l = range(0, n)\n",
    "        k = int(0.6 * (n))\n",
    "        training_index = random.sample(l, k)\n",
    "        df_training = df_competition_evaluation.iloc[training_index]\n",
    "        test_index = [x for x in l if x not in training_index]\n",
    "        df_test = df_competition_evaluation.iloc[test_index]\n",
    "        glm_results = my_logistic_regression(df_training)\n",
    "\n",
    "        # calculate the raw predictions on the test set\n",
    "        a0 = glm_results[0]\n",
    "        a1 = glm_results[1]\n",
    "        a2 = glm_results[2]\n",
    "        a3 = glm_results[3]\n",
    "\n",
    "        glm_pred = 1 / \\\n",
    "            (1 + np.exp(-(a0 + a1 * df_test['V1'] +\n",
    "             a2 * df_test['V2'] + a3 * df_test['V3'])))\n",
    "\n",
    "        # calculate the ROC\n",
    "        TPR = []\n",
    "        FPR = []\n",
    "        thresholds = np.arange(0, 1.001, 0.001)\n",
    "        m = len(thresholds)\n",
    "        pd.options.mode.chained_assignment = None  # default='warn'\n",
    "        df_test['glm_pred'] = glm_pred\n",
    "\n",
    "        for threshold in thresholds:\n",
    "            TP = len(df_test[(df_test['Target'] == 1) &\n",
    "                             (df_test['glm_pred'] > threshold)])\n",
    "            FN = len(df_test[(df_test['Target'] == 1) &\n",
    "                             (df_test['glm_pred'] < threshold)])\n",
    "            FP = len(df_test[(df_test['Target'] == 0) &\n",
    "                             (df_test['glm_pred'] > threshold)])\n",
    "            TN = len(df_test[(df_test['Target'] == 0) &\n",
    "                             (df_test['glm_pred'] < threshold)])\n",
    "            TPR.append(TP / (TP + FN))\n",
    "            FPR.append(FP / (FP + TN))\n",
    "\n",
    "        TPR[0] = 1\n",
    "        FPR[0] = 1\n",
    "        TPR[m - 1] = 0\n",
    "        FPR[m - 1] = 0\n",
    "\n",
    "        # Integrating the ROC to get the AUC\n",
    "        c = np.array([sum(n) / 2 for n in zip(*[TPR[0:(m - 1)], TPR[1:m]])])\n",
    "        AUC = abs(round(sum(np.diff(FPR) * c), 3))\n",
    "        AUC_list.append(AUC)\n",
    "\n",
    "    AUC_avgs.append(np.mean(AUC_list))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
