{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def my_logistic_regression(df_competition):\n",
    "    \"\"\"\n",
    "    Computes\n",
    "    \"\"\"\n",
    "\n",
    "    # format data\n",
    "    data = df_competition.values  # convert data frame to numpy array\n",
    "    m, n = data[:, :-1].shape\n",
    "    X = np.concatenate((np.ones((m, 1)), data[:, :-1]), axis=1)\n",
    "    y = data[:, -1]\n",
    "\n",
    "    # calculate theta\n",
    "    initial_theta = np.zeros(X.shape[1])  # initialize theta to all 0s\n",
    "    alpha = 0.01  # set learning rate\n",
    "    numIterations = 10000  # set number of iterations\n",
    "    theta, cost_j = gradientDescent(X, y, initial_theta, alpha, numIterations)\n",
    "    return theta\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Computes the sigmoid function for all values of z.\n",
    "\n",
    "    :param z: scalar, vector or array of values for which the sigmoid function should be computed\n",
    "    :returns: scalar, vector or array of values of size z containing computed sigmoid values\n",
    "    \"\"\"\n",
    "    return (1 / (1 + np.power(np.e, -z)))\n",
    "\n",
    "\n",
    "def costFunction(theta, X, y):\n",
    "    \"\"\"\n",
    "    Computes the cost of using theta as the parameter for logistic regression and the\n",
    "    gradient of the cost w.r.t. to the parameters\n",
    "\n",
    "    :param theta: parameter vector containing corresponding values of theta\n",
    "    :param X: MxN array that contains feature set (x-values)\n",
    "    :param y: Mx1 array that contains resulting outcomes (y-values)\n",
    "    :returns: a scalar of the cost \"J\" and gradient \"grad\" of the cost with the same size as theta\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    h = sigmoid(np.dot(X, theta))  # get predictions\n",
    "    J = -(1 / m) * (np.log(h).T.dot(y) +\n",
    "                    np.log(1 - h).T.dot(1 - y))  # calculate cost\n",
    "\n",
    "    return J\n",
    "\n",
    "\n",
    "def calculateGradient(theta, X, y):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the cost with respect to cost J and theta.\n",
    "\n",
    "    :param theta: parameter vector containing corresponding values of theta\n",
    "    :param X: MxN array that contains feature set (x-values)\n",
    "    :param y: Mx1 array that contains resulting outcomes (y-values)\n",
    "    :returns: gradient of the cost with the same size as theta\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    h = sigmoid(np.dot(X, theta))\n",
    "    return (1 / m) * np.dot(X.transpose(), h - y)\n",
    "\n",
    "def gradientDescent(X, y, theta, alpha, numIterations):\n",
    "    \"\"\"\n",
    "    Runs gradient descent to optimize a cost function for linear regression\n",
    "    and returns the optimal parameter values theta.\n",
    "\n",
    "    :param X: MxN array that contains feature set (x-values)\n",
    "    :param y: Mx1 array that contains resulting outcomes (y-values)\n",
    "    :param alpha: scalar value that defines the learning rate for gradient descent\n",
    "    :param theta: parameter vector containing corresponding values of theta\n",
    "    :returns: optimal parameter set \"theta\" and an array \"J_history\" containing the values of J\n",
    "    for each iteration\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    J_history = np.zeros(numIterations)\n",
    "\n",
    "    for i in range(0, numIterations):\n",
    "        pred = np.dot(X, theta)  # get predictions\n",
    "        loss = pred - y  # calculate loss\n",
    "        J_history[i] = costFunction(theta, X, y)  # calculate cost\n",
    "        gradient = calculateGradient(theta, X, y)\n",
    "        theta = theta - (alpha * gradient)  # update theta\n",
    "\n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using scipy.optimize.minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fun: 0.5862879674134049\n",
      " hess_inv: array([[ 4.52510031, -0.10246485,  0.54554132, -0.95214004],\n",
      "       [-0.10246485,  4.29084897,  1.63887751,  0.68874903],\n",
      "       [ 0.54554132,  1.63887751,  4.28118445,  0.3694915 ],\n",
      "       [-0.95214004,  0.68874903,  0.3694915 ,  6.48771912]])\n",
      "      jac: array([ -2.57812700e-06,  -5.19993812e-06,   6.88059287e-06,\n",
      "         6.77566468e-07])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 16\n",
      "      nit: 15\n",
      "     njev: 16\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([-0.12866398,  0.29137066,  0.85169036,  0.61401509])\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "df_competition = pd.read_csv('C:/Users/802161/Source/github/glm-competition/data/df_competition.csv')\n",
    "\n",
    "# format data\n",
    "data = df_competition.values  # convert data frame to numpy array\n",
    "m, n = data[:, :-1].shape\n",
    "X = np.concatenate((np.ones((m, 1)), data[:, :-1]), axis=1)\n",
    "y = data[:, -1]\n",
    "\n",
    "# optimize theta\n",
    "initial_theta = np.zeros(X.shape[1])\n",
    "res = minimize(costFunction, initial_theta, args=(X, y),\n",
    "           method=None, jac=calculateGradient, options={'maxiter': 1000})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_competition_evaluation = pd.read_csv('C:/Users/802161/Source/github/glm-competition/data/df_competition.csv')\n",
    "\n",
    "# listed of average AUCs\n",
    "AUC_avgs = []\n",
    "\n",
    "for avg in range(0, 10):\n",
    "    # list of calculated AUCs\n",
    "    AUC_list = []\n",
    "    \n",
    "    for iteration in range(0, 100):\n",
    "        n = len(df_competition_evaluation)\n",
    "\n",
    "        # divide into training and test\n",
    "        l = range(0, n)\n",
    "        k = int(0.6 * (n))\n",
    "        training_index = random.sample(l, k)\n",
    "        df_training = df_competition_evaluation.iloc[training_index]\n",
    "        test_index = [x for x in l if x not in training_index]\n",
    "        df_test = df_competition_evaluation.iloc[test_index]\n",
    "        glm_results = my_logistic_regression(df_training)\n",
    "\n",
    "        # calculate the raw predictions on the test set\n",
    "        a0 = glm_results[0]\n",
    "        a1 = glm_results[1]\n",
    "        a2 = glm_results[2]\n",
    "        a3 = glm_results[3]\n",
    "\n",
    "        glm_pred = 1 / \\\n",
    "            (1 + np.exp(-(a0 + a1 * df_test['V1'] +\n",
    "             a2 * df_test['V2'] + a3 * df_test['V3'])))\n",
    "\n",
    "        # calculate the ROC\n",
    "        TPR = []\n",
    "        FPR = []\n",
    "        thresholds = np.arange(0, 1.001, 0.001)\n",
    "        m = len(thresholds)\n",
    "        pd.options.mode.chained_assignment = None  # default='warn'\n",
    "        df_test['glm_pred'] = glm_pred\n",
    "\n",
    "        for threshold in thresholds:\n",
    "            TP = len(df_test[(df_test['Target'] == 1) &\n",
    "                             (df_test['glm_pred'] > threshold)])\n",
    "            FN = len(df_test[(df_test['Target'] == 1) &\n",
    "                             (df_test['glm_pred'] < threshold)])\n",
    "            FP = len(df_test[(df_test['Target'] == 0) &\n",
    "                             (df_test['glm_pred'] > threshold)])\n",
    "            TN = len(df_test[(df_test['Target'] == 0) &\n",
    "                             (df_test['glm_pred'] < threshold)])\n",
    "            TPR.append(TP / (TP + FN))\n",
    "            FPR.append(FP / (FP + TN))\n",
    "\n",
    "        TPR[0] = 1\n",
    "        FPR[0] = 1\n",
    "        TPR[m - 1] = 0\n",
    "        FPR[m - 1] = 0\n",
    "\n",
    "        # Integrating the ROC to get the AUC\n",
    "        c = np.array([sum(n) / 2 for n in zip(*[TPR[0:(m - 1)], TPR[1:m]])])\n",
    "        AUC = abs(round(sum(np.diff(FPR) * c), 3))\n",
    "        AUC_list.append(AUC)\n",
    "\n",
    "    AUC_avgs.append(np.mean(AUC_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
